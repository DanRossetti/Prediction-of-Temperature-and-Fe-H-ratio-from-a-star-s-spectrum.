{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import library"
      ],
      "metadata": {
        "id": "J2tJF89_PuGv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3NLaWxnPvwC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33aca803-e497-4e79-8bdb-33bf31c9bfa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/176.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m153.6/176.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.1/176.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install keras_tuner -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FncYfc9Q5xSH"
      },
      "outputs": [],
      "source": [
        "# arrays\n",
        "import numpy as np\n",
        "\n",
        "# fits\n",
        "from astropy.io import fits\n",
        "from astropy.utils.data import download_file\n",
        "from astropy.visualization import simple_norm\n",
        "from astropy.table import Table\n",
        "\n",
        "# plotting\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# keras\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout, LSTM, Input, GRU\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from keras.optimizers import SGD, Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.datasets import load_iris\n",
        "from numpy import unique\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "import keras_tuner as kt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend as K\n",
        "\n",
        "# sklearn (for machine learning)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "\n",
        "#Drive\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTTMsH_sql2o"
      },
      "outputs": [],
      "source": [
        "from keras.layers import  BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40rYjJCqP5US"
      },
      "source": [
        "#Some usefull function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plotta_gaussiana_errore(pred1, pred2, pred3, label1, label2, label3):\n",
        "    # Compute errors between prediction and label\n",
        "    error1 = pred1 - label1\n",
        "    error2 = pred2 - label2\n",
        "    error3 = pred3 - label3\n",
        "\n",
        "    # Calculate the mean and standard deviation of the errors\n",
        "    mean1 = np.mean(error1)\n",
        "    std1 = np.std(error1)\n",
        "    mean2 = np.mean(error2)\n",
        "    std2 = np.std(error2)\n",
        "    mean3 = np.mean(error3)\n",
        "    std3 = np.std(error3)\n",
        "    # Generate a range of x values for the plot\n",
        "    x = np.arange(-5000, 5000, 500)\n",
        "\n",
        "    # Calculate the Gaussian curves of the errors\n",
        "    y1 = (1 / (std1 * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mean1) / std1) ** 2)\n",
        "    y2 = (1 / (std2 * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mean2) / std2) ** 2)\n",
        "    y3 = (1 / (std3 * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mean3) / std3) ** 2)\n",
        "\n",
        "    # Plot the Gaussian curves of the errors\n",
        "    plt.figure(figsize=(16, 10))\n",
        "    plt.plot(x, y1, label='All Stars', color='green')\n",
        "    plt.plot(x, y2, label='Blue Stars', color='blue')\n",
        "    plt.plot(x, y3, label='Red Stars', color='red')\n",
        "    plt.axvline(x=mean1, color='green', linestyle='--', label='Mean Error all stars')\n",
        "    plt.axvline(x=mean2, color='blue', linestyle='--', label='Mean Error Blue stars')\n",
        "    plt.axvline(x=mean3, color='red', linestyle='--', label='Mean Error Red stars')\n",
        "    plt.xlabel('Error')\n",
        "    plt.ylabel('Probability distribution')\n",
        "    plt.title('Error distribution')\n",
        "    plt.legend()\n",
        "    plt.xticks(x)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "1LRj7UZiOdvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikcyvqQkCnGP"
      },
      "outputs": [],
      "source": [
        "def calcola_errore(pred, gt):\n",
        "    # Calculate the error as the difference between predictions and ground truth\n",
        "    error = pred - gt\n",
        "    # Calculate the mean error\n",
        "    mean_error = np.mean(error)\n",
        "    # Calculate the variance of the error\n",
        "    error_variance = np.var(error)\n",
        "    return mean_error, error_variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HDe0BVNEIRq"
      },
      "outputs": [],
      "source": [
        "def filter_and_plot(predictions, labels, y_bassa=0, y_alta=1):\n",
        "    x= np.linspace(y_bassa, y_alta, 1000)\n",
        "    plt.figure(figsize=(16, 16))\n",
        "    plt.grid()\n",
        "    plt.plot(x, x, label='Linea di riferimento')\n",
        "    plt.plot(labels, predictions, 'x', label='Valori predetti')\n",
        "    plt.xlabel('Fe/H reale')\n",
        "    plt.ylabel('Fe/H predetto')\n",
        "    plt.title('Confronto tra valori predetti e reali per Fe/H')\n",
        "    plt.ylim(y_bassa, y_alta)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEowMk7k6BYr"
      },
      "source": [
        "Load the observed stellar spectra, each of them is labelled with a temperature and Fe/H\n",
        "The CNN will be trained to predict the stellar temperature values."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount drive"
      ],
      "metadata": {
        "id": "qI3PbeyRRKta"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8CrincX52qZ",
        "outputId": "3c07940b-1823-4859-e67e-fd6555f55487"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive/', force_remount=True)\n",
        "path_data = \"/content/drive/MyDrive/Colab Notebooks/environment/Galaxi marging/Telescope Data/Spectroscopy CNN/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnYxy2scQorr",
        "outputId": "aaabfdac-b6f9-4249-cafb-d40cb9b1e831"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/content/drive/MyDrive/Colab Notebooks/environment/22-03-23EX/Telescope Data/Spectroscopy CNN': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls \"/content/drive/MyDrive/Colab Notebooks/environment/22-03-23EX/Telescope Data/Spectroscopy CNN\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofQHkCAxqeSt"
      },
      "source": [
        "#read the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wg4w0s696ihh"
      },
      "outputs": [],
      "source": [
        "file_name = 'XSL_labels.fits'\n",
        "hdu = fits.open(path_data+file_name)\n",
        "Labels = Table(hdu[1].data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POYOc965T2OD"
      },
      "source": [
        "The spectra are sampled in 1000 pixels from 350 to 950 nm, and are normalized to 1 at 500 nm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAq6VT4EUX8p"
      },
      "outputs": [],
      "source": [
        "#Read the spectra\n",
        "hdu = fits.open(path_data+'XSHOOTER_spec_lib.fits')\n",
        "spectra = hdu[0].data\n",
        "wave = np.linspace(350,950,1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-gxC3C4PQKF"
      },
      "outputs": [],
      "source": [
        "# set the random seed to get the same random set of images each time, or comment it out to get different ones!\n",
        "np.random.seed(101)\n",
        "\n",
        "# select 16 random image indices:\n",
        "example_ids = np.random.choice(len(spectra), 4)\n",
        "print(Labels['Teff'][example_ids])\n",
        "\n",
        "# initialize your figure\n",
        "fig, ax= plt.subplots(nrows=2, ncols=1, figsize=(14, 5))\n",
        "\n",
        "# loop through the randomly selected images and plot with labels\n",
        "for i in range(2):\n",
        "    ax[i].plot(wave,spectra[example_ids[2*i+0],:,0])\n",
        "    ax[i].plot(wave,spectra[example_ids[2*i+1],:,0])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaS2zHst7p2G"
      },
      "source": [
        "We will denote the input spectra as X and their corresponding labels (temperature) as y, following the convention used by sklearn.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIUU2yg6oSnO"
      },
      "outputs": [],
      "source": [
        "# Save the spectra and labels for the external temperature and iron-hydrogen ratio\n",
        "X = spectra\n",
        "Y = np.array(Labels['Teff'])\n",
        "Y_FH= np.array(Labels['__Fe_H_'])\n",
        "\n",
        "# Split the red stars from the blue stars and their respective labels\n",
        "stelle_rosse = X[np.where(Y < 4000)[0]]\n",
        "stelle_azzurre = X[np.where(Y >4000)[0]]\n",
        "\n",
        "#temperature\n",
        "Teff_rosse = Y[np.where(Y < 4000)[0]]\n",
        "Teff_azzurre = Y[np.where(Y > 4000)[0]]\n",
        "\n",
        "#iron-hydrogen ratio\n",
        "FH_rosse = Y_FH[np.where(Y < 4000)[0]]\n",
        "FH_azzurre = Y_FH[np.where(Y > 4000)[0]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 1))\n",
        "#plot histogram of Teff\n",
        "plt.hist([Y], bins=1000, label=['Teff'], color='red')\n",
        "plt.legend()\n",
        "plt.xlabel('Valore')\n",
        "plt.ylabel('Frequenza')\n",
        "plt.ylim(0, 20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bKX3pyaG9tVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 1))\n",
        "# Plot histogram of Fe/H\n",
        "plt.hist([Y_FH], bins=1000, label=['Fe/H'], color='red')\n",
        "plt.legend()\n",
        "plt.xlabel('Valore')\n",
        "plt.ylabel('Frequenza')\n",
        "plt.ylim(0, 20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FInDRgu1-iE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXj2NotO68sm"
      },
      "outputs": [],
      "source": [
        "#Nomalize Teff between 0 and 1\n",
        "norm = np.max(Labels['Teff'])\n",
        "Y_norm = Y/norm\n",
        "Teff_rosse = Teff_rosse/ norm\n",
        "Teff_azzurre = Teff_azzurre / norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOf_aEs0m-AN"
      },
      "outputs": [],
      "source": [
        "#find the nan values and remuve them\n",
        "bad = np.any(np.isnan(X[:,:,0]), axis=1)\n",
        "bad_rosse = np.any(np.isnan(stelle_rosse[:,:,0]), axis=1)\n",
        "bad_azzurre = np.any(np.isnan(stelle_azzurre[:,:,0]), axis=1)\n",
        "#delete variables unsuefull\n",
        "X = X[~bad,...]\n",
        "Y_norm = Y_norm[~bad,...]\n",
        "Y_FH = Y_FH[~bad, ...]\n",
        "stelle_rosse = stelle_rosse[~bad_rosse,...]\n",
        "stelle_azzurre = stelle_azzurre[~bad_azzurre,...]\n",
        "FH_rosse = FH_rosse[~bad_rosse,...]\n",
        "FH_azzurre = FH_azzurre[~bad_azzurre,...]\n",
        "Teff_rosse = Teff_rosse[~bad_rosse,...]\n",
        "Teff_azzurre = Teff_azzurre[~bad_azzurre,...]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD80jIwokqmU"
      },
      "outputs": [],
      "source": [
        "#split for teff\n",
        "training_data_X, testing_data_X, training_data_Y, testing_data_Y = train_test_split(X, Y_norm, test_size=0.2, random_state=42)\n",
        "#Split for FH\n",
        "train_FH, test_FH, train_y_FH, test_y_FH = train_test_split(X, Y_FH, test_size=0.2, random_state=42)\n",
        "#split for FH rosse e azzurre\n",
        "train_rosse_FH, test_rosse_FH, y_train_rosse_FH, y_test_rosse_FH = train_test_split(stelle_rosse, FH_rosse, test_size=0.2, random_state=42)\n",
        "train_azzurre_FH, test_azzurre_FH, y_train_azzurre_FH, y_test_azzurre_FH = train_test_split(stelle_azzurre, FH_azzurre, test_size=0.2, random_state=42)\n",
        "series_size = len(training_data_X[0,:,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-iBXqfA1QU6"
      },
      "outputs": [],
      "source": [
        "#Normalize Fe/H values between 0 and 1\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "scaler_FH = MinMaxScaler()\n",
        "train_y_FH = train_y_FH.reshape(-1, 1)\n",
        "test_y_FH = test_y_FH.reshape(-1,1)\n",
        "y_test_rosse_FH = y_test_rosse_FH.reshape(-1,1)\n",
        "y_test_azzurre_FH = y_test_azzurre_FH.reshape(-1,1)\n",
        "scaler_FH.fit(train_y_FH)\n",
        "\n",
        "train_y_FH = scaler_FH.transform(train_y_FH)\n",
        "test_y_FH = scaler_FH.transform(test_y_FH)\n",
        "\n",
        "y_test_rosse_FH = scaler_FH.transform(y_test_rosse_FH)\n",
        "y_test_azzurre_FH = scaler_FH.transform(y_test_azzurre_FH)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEmcCKnBQAhq"
      },
      "source": [
        "#Prediction of Teff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gGe-wUdqWlP"
      },
      "source": [
        "define model with only convolutional layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUvSsrAKadWz"
      },
      "outputs": [],
      "source": [
        "def define_model(initial_input_shape):\n",
        "  model = Sequential()\n",
        "  model.add(Conv1D(3, 15, activation='relu',  padding='same', input_shape=initial_input_shape))\n",
        "  model.add(MaxPooling1D(2))\n",
        "  model.add(Conv1D(3, 13, activation='relu',  padding='same' ) )\n",
        "  model.add(Flatten(name='flatten_layer'))\n",
        "  layer = model.get_layer('flatten_layer')\n",
        "  model.add(Dense(layer.output_shape[1], activation='relu'))\n",
        "  model.add(Dense(1, activation='linear'))\n",
        "  opt = Adam(learning_rate=0.0001)\n",
        "  model.compile(optimizer=opt, loss=tf.keras.losses.MeanAbsoluteError(), metrics=['mae']) #now compile the model with all the ingredients\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "input_shape = np.shape(training_data_X[1])\n",
        "CNN = define_model(input_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArbmWC3uQMa3"
      },
      "source": [
        "define model with lstm layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtXW6-viT6TU"
      },
      "outputs": [],
      "source": [
        "def define_model(initial_input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(3, 15, activation='relu', padding='same', input_shape=initial_input_shape))\n",
        "    model.add(MaxPooling1D(2))\n",
        "    model.add(Conv1D(3, 13, activation='relu', padding='same'))\n",
        "    model.add(LSTM(32, activation='relu', return_sequences=True))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=opt, loss=tf.keras.losses.MeanAbsoluteError(), metrics=['mae'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "input_shape = np.shape(training_data_X[1])\n",
        "CNN = define_model(input_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYGJ0uhNS3Bo"
      },
      "source": [
        "define model with Gru"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edeIQjypS2WW"
      },
      "outputs": [],
      "source": [
        "def define_model(initial_input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(3, 15, activation='relu', padding='same', input_shape=initial_input_shape))\n",
        "    model.add(MaxPooling1D(2))\n",
        "    model.add(Conv1D(3, 13, activation='relu', padding='same'))\n",
        "    model.add(GRU(32, activation='relu', return_sequences=True))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "    model.compile(optimizer=opt, loss=tf.keras.losses.MeanAbsoluteError(), metrics=['mae'])\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "input_shape = np.shape(training_data_X[1])\n",
        "CNN = define_model(input_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "model with only fully connetted layer"
      ],
      "metadata": {
        "id": "kLaBhyzwfH_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def define_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, activation='relu', input_shape=input_shape))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=opt, loss='mae', metrics=['mae'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "input_shape = np.shape(training_data_X[1])\n",
        "CNN = define_model(input_shape)"
      ],
      "metadata": {
        "id": "KwjTfqOwfHIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trasformer"
      ],
      "metadata": {
        "id": "cXd3BDQVfN8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "td6m2qEvfM19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Dropout, LayerNormalization\n",
        "from tensorflow.keras.activations import gelu\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    # Calculate the dot product between query and key matrices\n",
        "    matmul_qk = K.batch_dot(q, k, axes=[2, 2])\n",
        "    # Get the dimension of the key matrix\n",
        "    dk = K.cast(K.shape(k)[-1], dtype=K.floatx())\n",
        "    # Scale the attention logits by the square root of dk\n",
        "    scaled_attention_logits = matmul_qk / K.sqrt(dk)\n",
        "    # Apply mask to attention logits if provided\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "    # Apply softmax to obtain attention weights\n",
        "    attention_weights = K.softmax(scaled_attention_logits, axis=-1)\n",
        "    # Calculate the weighted sum of the value matrix\n",
        "    output = K.batch_dot(attention_weights, v)\n",
        "    return output, attention_weights\n",
        "\n",
        "def transformer_encoder(inputs, d_model, num_heads, ff_dim, dropout):\n",
        "    # Multi-head self-attention mechanism\n",
        "    attention = inputs\n",
        "    attention = Dense(d_model, activation=gelu)(attention)\n",
        "    attention = Dropout(dropout)(attention)\n",
        "    attention = Dense(d_model, activation=gelu)(attention)\n",
        "    attention = Dropout(dropout)(attention)\n",
        "    attention = Dense(d_model, activation=gelu)(attention)\n",
        "    attention = Dropout(dropout)(attention)\n",
        "\n",
        "    # Apply scaled dot-product attention\n",
        "    attention, _ = scaled_dot_product_attention(attention, attention, attention)\n",
        "\n",
        "    # Residual connection and layer normalization\n",
        "    outputs = LayerNormalization(epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "    # Feed-forward neural network\n",
        "    ffn = outputs\n",
        "    ffn = Dense(ff_dim, activation=gelu)(ffn)\n",
        "    ffn = Dropout(dropout)(ffn)\n",
        "    ffn = Dense(d_model)(ffn)\n",
        "    ffn = Dropout(dropout)(ffn)\n",
        "\n",
        "    # Residual connection and layer normalization\n",
        "    outputs = LayerNormalization(epsilon=1e-6)(outputs + ffn)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "def define_model(input_shape, d_model, num_heads, ff_dim, num_encoders, dropout):\n",
        "    # Define the input layer\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Apply transformer encoder blocks\n",
        "    x = inputs\n",
        "    for _ in range(num_encoders):\n",
        "        x = transformer_encoder(x, d_model, num_heads, ff_dim, dropout)\n",
        "\n",
        "    # Extract the final output from the last encoder block\n",
        "    outputs = Dense(1, activation='linear')(x[:, -1, :])\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    # Compile the model with optimizer, loss function, and metrics\n",
        "    model.compile(optimizer = opt, loss='mae', metrics=['mae'])\n",
        "\n",
        "    # Print model summary\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "input_shape = np.shape(training_data_X[1]) # Shape of the spectra\n",
        "d_model = 64  # Dimension of the model\n",
        "num_heads = 10  # Number of heads of attention\n",
        "ff_dim = 128  # Dimension of feed-forward layer\n",
        "num_encoders = 2  # Number of encoders in the Transformer\n",
        "dropout = 0.1  # Dropout value\n",
        "\n",
        "CNN = define_model(input_shape, d_model, num_heads, ff_dim, num_encoders, dropout)\n"
      ],
      "metadata": {
        "id": "f1j6UKeSfaKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xDrzkygZUU1"
      },
      "outputs": [],
      "source": [
        "n_epochs = 10\n",
        "batch_size = 8\n",
        "#fit the model\n",
        "history = CNN.fit(training_data_X, training_data_Y,\n",
        "                  batch_size=batch_size,\n",
        "                  validation_split=0.3,\n",
        "                  epochs=n_epochs, verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WE8afgJCao36"
      },
      "outputs": [],
      "source": [
        "#Plot loss and validation loss\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.title('Mean Average Error')\n",
        "plt.plot(history.history['loss'], color='blue', label='train')\n",
        "plt.plot(history.history['val_loss'], color='orange', label='validation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.clf()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kwUBjP4f1WO",
        "outputId": "6b618e2f-0a07-4797-c219-01b401252b44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 4s 1s/step\n"
          ]
        }
      ],
      "source": [
        "#make prediction\n",
        "y_pred= CNN.predict(testing_data_X, verbose = True, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJTq1w6JgEi1"
      },
      "outputs": [],
      "source": [
        "#plot prediction\n",
        "plt.figure(figsize=(10, 6))\n",
        "test_norm=testing_data_Y*norm\n",
        "pred_norm = y_pred*norm\n",
        "x=np.linspace(0,30000,1000)\n",
        "plt.grid()\n",
        "plt.plot(x,x)\n",
        "plt.plot(test_norm, pred_norm , 'x')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plot prediction whithout outliers\n",
        "filter_and_plot(pred_norm, test_norm, 0 , 30000)"
      ],
      "metadata": {
        "id": "5abjJHY_cDj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compute MAE\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "mae = mean_absolute_error(test_norm, pred_norm)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)"
      ],
      "metadata": {
        "id": "TWelen55li08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compute mean error and variance\n",
        "errore , varianza = calcola_errore(pred_norm, test_norm)\n",
        "errore, varianza"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5QZk43dC9XP",
        "outputId": "7ad5e1f2-9b74-418d-e2a5-92bd3cf471e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(712.6698019350536, 16912017.001289435)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Predict only Red stars"
      ],
      "metadata": {
        "id": "iLpVZ2kmMxlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#make prediction for red star\n",
        "y_pred_rosse = CNN.predict(stelle_rosse, verbose = True, batch_size=64)\n",
        "#denormalization outputs and labels\n",
        "y_pred_rosse =  y_pred_rosse*norm\n",
        "Teff_rosse = Teff_rosse * norm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CEhg8C3MzeE",
        "outputId": "c62ca748-9324-40d6-97ba-75e6bb8d9534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 2s 485ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compute mean error and variance\n",
        "errore , varianza = calcola_errore(y_pred_rosse, Teff_rosse)\n",
        "errore, varianza"
      ],
      "metadata": {
        "id": "MKjHy-DcNw6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Predic only not red stars"
      ],
      "metadata": {
        "id": "fdISNEFPN09e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#compute outputs\n",
        "y_pred_azzurre = CNN.predict(stelle_azzurre, verbose = True, batch_size=64)\n",
        "#denormalization outputs and labels\n",
        "y_pred_azzurre = y_pred_azzurre* norm\n",
        "Teff_azzurre = Teff_azzurre*norm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-9ODQZUN4oa",
        "outputId": "ab073e4d-fb31-40ae-cf88-80ecbfa1d6be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 3s 387ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compute mean error and variance\n",
        "errore , varianza = calcola_errore(y_pred_azzurre, Teff_azzurre)\n",
        "errore, varianza"
      ],
      "metadata": {
        "id": "YYplE0w3N5Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot error\n",
        "plotta_gaussiana_errore(pred_norm,  y_pred_azzurre, y_pred_rosse, test_norm, Teff_azzurre, Teff_rosse)"
      ],
      "metadata": {
        "id": "Rtm4lOJxOS4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmxrqX5SqQI3"
      },
      "source": [
        "##Hyperparameter Tuning for convolutional layer for Teff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPmIdHtdQgnU"
      },
      "source": [
        "define model for the hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQs6ZH2jqnzO"
      },
      "outputs": [],
      "source": [
        "def def_model(hp):\n",
        "\n",
        "  model = Sequential()\n",
        "  hp_Num_Kernel=hp.Int('kernel', min_value=3, max_value=21, step=2)\n",
        "  hp_kernel_size=hp.Int('kernel_size',min_value=3, max_value=17, step=2)\n",
        "  model.add(Conv1D(hp_Num_Kernel,hp_kernel_size, activation='relu',  padding='same', input_shape=input_shape))\n",
        "\n",
        "  hp_Num_MaxPool=hp.Int('maxpoolSize', min_value=2, max_value=4, step=2)\n",
        "  model.add(MaxPooling1D(2))\n",
        "\n",
        "  hp_Num_Kernel1=hp.Int('kernel1', min_value=3, max_value=21, step=2)\n",
        "  hp_kernel_size1=hp.Int('kernel_size1',min_value=3, max_value=17, step=2)\n",
        "  model.add(Conv1D(hp_Num_Kernel1,hp_kernel_size1, activation='relu',  padding='same'))\n",
        "\n",
        "  model.add(Flatten(name='flatten_layer')) #now we flat the maxpooling output to match the fully connected Dense layer input\n",
        "  layer = model.get_layer('flatten_layer')\n",
        "\n",
        "  model.add(Dense(layer.output_shape[1], activation='relu'))\n",
        "\n",
        "  #scelta la sigmoide perche classificazione solo si o no e non con molte classi\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  hp_learningRate=hp.Float('learning rate', min_value=0.001, max_value=0.01, step=0.02 )\n",
        "  opt = SGD(hp_learningRate)\n",
        "\n",
        "  model.compile(optimizer=opt, loss=tf.keras.losses.MeanSquaredError(), metrics=['mse']) #now compile the model with all the ingredients\n",
        "  model.summary()\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PvpUx3mrdov"
      },
      "outputs": [],
      "source": [
        "tuner = kt.Hyperband(def_model, objective='val_loss', max_epochs=5, factor=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8sWQy2Qrt6B"
      },
      "outputs": [],
      "source": [
        "tuner.search_space_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSIYo-9tryp5"
      },
      "outputs": [],
      "source": [
        "stop_early= tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emOuy3zUr2XI"
      },
      "outputs": [],
      "source": [
        "n_epochs = 100\n",
        "#search of best parameters\n",
        "tuner.search(training_data_X,training_data_Y,batch_size=batch_size, epochs=n_epochs, validation_split=0.2, callbacks=[stop_early] )\n",
        "best_hp=tuner.get_best_hyperparameters()[0]\n",
        "hmodel=tuner.hypermodel.build(best_hp)\n",
        "hmodel.summary()\n",
        "#values of best model\n",
        "best_hp.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkwMiT1hucNj"
      },
      "outputs": [],
      "source": [
        "#fit model\n",
        "history=hmodel.fit(training_data_X,training_data_Y,\n",
        "                  batch_size=batch_size,\n",
        "                  validation_split=0.3,\n",
        "                  epochs=n_epochs, callbacks=stop_early, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOSetLjgvOuB",
        "outputId": "a0d218bf-31b4-4eae-e5f9-5bd173b84b5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5/5 [==============================] - 0s 16ms/step\n"
          ]
        }
      ],
      "source": [
        "#make outputs\n",
        "pred= hmodel.predict(testing_data_X)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plot prediction\n",
        "plt.figure(figsize=(10, 6))\n",
        "test_norm = testing_data_Y*norm\n",
        "pred_norm = y_pred*norm\n",
        "x = np.linspace(0,30000,1000)\n",
        "plt.grid()\n",
        "plt.plot(x,x)\n",
        "plt.plot(test_norm, pred_norm , 'x')"
      ],
      "metadata": {
        "id": "0BW7oJ7LviMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot prediction whithout outliers\n",
        "filter_and_plot(pred_norm, test_norm, 0 , 30000)"
      ],
      "metadata": {
        "id": "WEbW6dJdvmbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O55i4_cUoksl"
      },
      "source": [
        "#Prediction of Fe H"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trasformer"
      ],
      "metadata": {
        "id": "foFnCt3cSR2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Dropout, LayerNormalization\n",
        "from tensorflow.keras.activations import gelu\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    # Calculate the dot product between query and key matrices\n",
        "    matmul_qk = K.batch_dot(q, k, axes=[2, 2])\n",
        "    # Get the dimension of the key matrix\n",
        "    dk = K.cast(K.shape(k)[-1], dtype=K.floatx())\n",
        "    # Scale the attention logits by the square root of dk\n",
        "    scaled_attention_logits = matmul_qk / K.sqrt(dk)\n",
        "    # Apply mask to attention logits if provided\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "    # Apply softmax to obtain attention weights\n",
        "    attention_weights = K.softmax(scaled_attention_logits, axis=-1)\n",
        "    # Calculate the weighted sum of the value matrix\n",
        "    output = K.batch_dot(attention_weights, v)\n",
        "    return output, attention_weights\n",
        "\n",
        "def transformer_encoder(inputs, d_model, num_heads, ff_dim, dropout):\n",
        "    # Multi-head self-attention mechanism\n",
        "    attention = inputs\n",
        "    attention = Dense(d_model, activation=gelu)(attention)\n",
        "    attention = Dropout(dropout)(attention)\n",
        "    attention = Dense(d_model, activation=gelu)(attention)\n",
        "    attention = Dropout(dropout)(attention)\n",
        "    attention = Dense(d_model, activation=gelu)(attention)\n",
        "    attention = Dropout(dropout)(attention)\n",
        "\n",
        "    # Apply scaled dot-product attention\n",
        "    attention, _ = scaled_dot_product_attention(attention, attention, attention)\n",
        "\n",
        "    # Residual connection and layer normalization\n",
        "    outputs = LayerNormalization(epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "    # Feed-forward neural network\n",
        "    ffn = outputs\n",
        "    ffn = Dense(ff_dim, activation=gelu)(ffn)\n",
        "    ffn = Dropout(dropout)(ffn)\n",
        "    ffn = Dense(d_model)(ffn)\n",
        "    ffn = Dropout(dropout)(ffn)\n",
        "\n",
        "    # Residual connection and layer normalization\n",
        "    outputs = LayerNormalization(epsilon=1e-6)(outputs + ffn)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "def define_model(input_shape, d_model, num_heads, ff_dim, num_encoders, dropout):\n",
        "    # Define the input layer\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Apply transformer encoder blocks\n",
        "    x = inputs\n",
        "    for _ in range(num_encoders):\n",
        "        x = transformer_encoder(x, d_model, num_heads, ff_dim, dropout)\n",
        "\n",
        "    # Extract the final output from the last encoder block\n",
        "    outputs = Dense(1, activation='linear')(x[:, -1, :])\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    # Compile the model with optimizer, loss function, and metrics\n",
        "    model.compile(optimizer = opt, loss='mae', metrics=['mae'])\n",
        "\n",
        "    # Print model summary\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "input_shape = np.shape(training_data_X[1]) # Shape of the spectra\n",
        "d_model = 64  # Dimension of the model\n",
        "num_heads = 10  # Number of heads of attention\n",
        "ff_dim = 128  # Dimension of feed-forward layer\n",
        "num_encoders = 2  # Number of encoders in the Transformer\n",
        "dropout = 0.1  # Dropout value\n",
        "\n",
        "CNN = define_model(input_shape, d_model, num_heads, ff_dim, num_encoders, dropout)"
      ],
      "metadata": {
        "id": "ivYCT9qjSMIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional Neural network"
      ],
      "metadata": {
        "id": "n5ykRthWSVJR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnIQwRYhvFIA"
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras.optimizers as keras_optimizers\n",
        "\n",
        "def create_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(3, 15, activation='relu', input_shape=input_shape))\n",
        "    model.add(MaxPooling1D(2))\n",
        "    model.add(Conv1D(3, 13, activation='relu'))\n",
        "    model.add(MaxPooling1D(2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "    model.compile(optimizer=opt, loss='mae', metrics=['mae'])\n",
        "\n",
        "    return model\n",
        "input_shape = np.shape(training_data_X[1])\n",
        "CNN = create_model(input_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional + LSTM"
      ],
      "metadata": {
        "id": "ZOUDa2Y3SaSG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSBlg5j5VE7f"
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras.optimizers as keras_optimizers\n",
        "def create_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(3, 15, activation='relu', input_shape=input_shape))\n",
        "    model.add(MaxPooling1D(2))\n",
        "    model.add(Conv1D(3, 13, activation='relu'))\n",
        "    model.add(MaxPooling1D(2))\n",
        "    model.add(LSTM(64, activation='relu', return_sequences=True))\n",
        "    model.add(LSTM(32, activation='relu'))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    opt = keras_optimizers.Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=opt, loss='mae', metrics=['mae'])\n",
        "\n",
        "    return model\n",
        "input_shape = np.shape(training_data_X[1])\n",
        "CNN = create_model(input_shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional + GRU"
      ],
      "metadata": {
        "id": "YzivRphaSdyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.optimizers as keras_optimizers\n",
        "def create_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(15, 3, activation='relu', input_shape=input_shape))\n",
        "    model.add(MaxPooling1D(2))\n",
        "    model.add(Conv1D(64, 3, activation='relu'))\n",
        "    model.add(MaxPooling1D(2))\n",
        "    model.add(GRU(64, activation='relu', return_sequences=True))\n",
        "    model.add(GRU(32, activation='relu'))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    opt = keras_optimizers.Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=opt, loss='mae', metrics=['mae'])\n",
        "\n",
        "    return model\n",
        "input_shape = np.shape(training_data_X[1])\n",
        "CNN = create_model(input_shape)"
      ],
      "metadata": {
        "id": "Qyxxr777eH8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fully connetted"
      ],
      "metadata": {
        "id": "euW0Rr9OShoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, activation='relu', input_shape=input_shape))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    opt = keras_optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "    model.compile(optimizer=opt, loss='mae', metrics=['mae'])\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "input_shape = np.shape(training_data_X[1])\n",
        "CNN = create_model(input_shape)"
      ],
      "metadata": {
        "id": "G_EkYEvThPcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXEdiIQio__5"
      },
      "outputs": [],
      "source": [
        "n_epochs = 100\n",
        "batch_size = 8\n",
        "#fit the model\n",
        "history = CNN.fit(train_FH, train_y_FH,\n",
        "                  batch_size=batch_size,\n",
        "                  validation_split=0.3,\n",
        "                  epochs=n_epochs, verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sbw6pGhWqHi6"
      },
      "outputs": [],
      "source": [
        "#plot loss and val loss\n",
        "n=0\n",
        "epochs = range(n, len(history.history['loss']))\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title('Cross Entropy Loss')\n",
        "plt.plot(epochs, history.history['loss'][n:], color='blue', label='train')\n",
        "plt.plot(epochs, history.history['val_loss'][n:], color='orange', label='validation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "plt.clf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KM69JLljrWBh",
        "outputId": "d3a8e6a5-fe7e-44ff-cc26-e7393804b497"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 116ms/step\n"
          ]
        }
      ],
      "source": [
        "#compute outputs\n",
        "y_pred= CNN.predict(test_FH, verbose = True, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOTnlY6jrlOZ"
      },
      "outputs": [],
      "source": [
        "#plot prediction\n",
        "x = np.linspace(0, 1, 10000)\n",
        "plt.figure(figsize=(16, 16))\n",
        "plt.grid()\n",
        "plt.plot(x, x, label='Linea di riferimento')\n",
        "plt.plot(test_y_FH, y_pred, 'x', label='Valori predetti')\n",
        "\n",
        "plt.xlabel('Rapporto Fe/H reale')\n",
        "plt.ylabel('Rapporto Fe/H predetto')\n",
        "plt.title('Confronto tra valori predetti e reali per il rapporto Fe/H')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKaOBo3c6XGn"
      },
      "outputs": [],
      "source": [
        "#denormalization of prediction and labels\n",
        "y_pred = scaler_FH.inverse_transform(y_pred)\n",
        "test_y_FH_naturali = scaler_FH.inverse_transform(test_y_FH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compute error and variance of prediction and labels\n",
        "errore , varianza = calcola_errore(y_pred, test_y_FH_naturali)\n",
        "errore, varianza"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iE8I7p7nK0Io",
        "outputId": "9db9fd49-218b-427e-e1c1-e9374289abc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.035783169059795436, 0.16632602795450982)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plot prediction without outliers\n",
        "filter_and_plot(y_pred, test_y_FH_naturali, -2, 2)"
      ],
      "metadata": {
        "id": "nEkKLE_D49mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute MAE\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "mae = mean_absolute_error(test_y_FH_naturali, y_pred)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)"
      ],
      "metadata": {
        "id": "2e0JNQim4cwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukJjcLtHluBD"
      },
      "source": [
        "##prediction only FeH Rosse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKyHkZZXmnfo"
      },
      "outputs": [],
      "source": [
        "#compute outputs only for red stars and denormalization of that\n",
        "y_pred_rosse = CNN.predict(train_rosse_FH, verbose = True, batch_size=64)\n",
        "y_pred_rosse = scaler_FH.inverse_transform(y_pred_rosse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_-FrWfwDPZX"
      },
      "outputs": [],
      "source": [
        "#compute error and variance\n",
        "errore , varianza = calcola_errore(y_pred_rosse, y_train_rosse_FH)\n",
        "errore, varianza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJZsaTmfmcEx"
      },
      "outputs": [],
      "source": [
        "#plot prediction\n",
        "x = np.linspace(0, 1, 10000)\n",
        "plt.figure(figsize=(16, 16))\n",
        "plt.grid()\n",
        "plt.plot(x, x, label='Linea di riferimento')\n",
        "plt.plot(y_test_rosse_FH, y_pred, 'x', label='Valori predetti')\n",
        "\n",
        "plt.xlabel('Rapporto Fe/H reale')\n",
        "plt.ylabel('Rapporto Fe/H predetto')\n",
        "plt.title('Confronto tra valori predetti e reali per il rapporto Fe/H')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJwDwwjyAKK9"
      },
      "outputs": [],
      "source": [
        "#plot prediction without outliers\n",
        "filter_and_plot(y_pred, y_test_rosse_FH, 2, segno= False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtFAKugDhr0z"
      },
      "source": [
        "## predict onlyFe/H of not red star"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bp2nFPgZiVe5",
        "outputId": "d35d464f-ae71-40e5-e46f-5bf07d273b41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 1s 176ms/step\n"
          ]
        }
      ],
      "source": [
        "#compute outputs only for red stars and denormalization of that\n",
        "y_pred_azzurre = CNN.predict(train_azzurre_FH, verbose = True, batch_size=64)\n",
        "y_pred_azzurre = scaler_FH.inverse_transform(y_pred_azzurre)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drpyeJw0Db16"
      },
      "outputs": [],
      "source": [
        "#compute error and variance\n",
        "errore , varianza = calcola_errore(y_pred_azzurre, y_train_azzurre_FH)\n",
        "errore, varianza"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZqYG5jiRF8S"
      },
      "source": [
        "##comparing the performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vp4VN1-VF9Dn"
      },
      "outputs": [],
      "source": [
        "#plot three error gaussian\n",
        "plotta_gaussiana_errore(y_pred,  y_pred_azzurre, y_pred_rosse, test_y_FH_naturali, y_train_azzurre_FH, y_train_rosse_FH)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "EmxrqX5SqQI3"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}